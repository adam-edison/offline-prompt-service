# Offline Prompt Service Configuration

# Directory where log files will be stored
# Default: logs (relative to project root)
LOGS_DIR=logs

# Ollama AI Model Configuration
# Base URL for Ollama API (required)
OLLAMA_BASE_URL=http://localhost:11434

# Default AI model to use (required)
# Examples: llama3.2:1b, gemma2:2b, qwen2.5:1.5b
OLLAMA_MODEL=llama3.2:1b